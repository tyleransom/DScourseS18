---
title: "PS8_Thatcher"
author: "Rachel Thatcher"
date: "March 14, 2018"
output: pdf_document
---

Question 4 - Create a data set
```{r}
#upload the package nloptr
library("nloptr")

#set the seed so results can be replicated
set.seed(100)

#create your data set - 100,000 by 10
N <- 100000
K <- 10
sigma <- .5

X <- matrix(rnorm(N*K, mean = 0, sd = sigma), N, K)

#make the first column all 1's
X[, 1] <- 1

#create eps - a vector of length N containing random numbers distributed  N(0, var) where sd = .5 
eps <- rnorm(N, mean = 0, sd = .5)

#create beta with length 10
beta <- as.vector(c(1.5, -1, -.25, .75, 3.5, -2, .5, 1, 1.25, 2))

#generate Y vector = X * beta + eps
Y <- X %*% beta + eps
```

Question 5 - Compute Beta OLS 
```{r}
beta.OLS <- solve(t(X) %*% X) %*% t(X) %*% Y
beta.OLS
```

Question 6 - Compute Beta OLS using gradient descent
```{r}
#step size (learning rate)
alpha <- .0000003

#number of iterations
max.iter <- 500000

#objective function
obj.func <- function(beta, Y, X){
  return ( sum(( Y - X %*% beta)^2) )
}

#define the gradient of objective function
gradient <- function(beta, Y, X){
  return ( as.vector(-2 * t(X) %*% (Y - X %*% beta)) )
}

#initial values - start at unfiorm random numbers equal to coefficients
beta <- runif(dim(X)[2])

#randomly initialize a value to beta
set.seed(100)

#vector to contain all beta's
beta.ALL <- matrix("numeric", length(beta), max.iter)

#gradient descent method to find the minimum
iter <- 1
beta0 <- 0*beta 
while (norm(as.matrix(beta0) - as.matrix(beta)) > 1e-8) {
  beta0 <- beta 
  beta <- beta0 - alpha * gradient(beta0, Y, X)
  beta.ALL[, iter] <- beta 
  if (iter%%10000 == 0) {
    print(beta)
  }
  iter <- iter + 1
}

#print result
print(iter)

#plot all xs for every iteration
print(paste("The minimum of f(beta, y, X) is ", beta, sep = ""))
```

Question 7 - Compute Beta OLS using nloptr's L-BFGS algorithm and then the Nelder-Mead algorithm.
```{r}
#use Objection function and gradient from question 6

#initial values - start at uniform random numbers equal to number of coefficients
beta0 <- runif(dim(X)[2])

#algoritm parameters
options <- list("algorithm" = "NLOPT_LD_LBFGS", 1.0e-6, "maxeval" = 1e3)

#optimize
results <- nloptr( x0 = beta0, eval_f = obj.func, eval_grad_f = gradient, opts = options, Y = Y, X = X)
print(results)

#now do it with Nelder-Mead

#algorithm parameters
options.2 <- list("algorithm" = "NLOPT_LN_NELDERMEAD", "xtol_rel" = 1.0e-8)

#optimize
results.2 <- nloptr( x0 = beta0, eval_f = obj.func, opts = options.2, Y = Y, X = X)
print(results.2)
```

Question 8 - Compute Beta MLE using nloptr's L-BFGS algorithm
```{r}
#ogjective funciton
obj.func.2 <- function(theta, Y, X) {
  beta <- theta[1:(length(theta) - 1)]
  sig <- theta[length(theta)]
  loglike <- -sum( -.5 * (log(2*pi*(sig^2)) + ((Y-X %*% beta)/sig)^2) )
  return (loglike)
}

#define gradient of objective funtion
gradient <- function(theta, Y, X) {
  grad <- as.vector(rep(0, length(theta)))
  beta <- theta[1:(length(theta) - 1)]
  sig <- theta[length(theta)]
  grad[1:(length(theta) -1)] <- -t(X) %*% (Y - X %*% beta) / (sig^2)
  grad[length(theta)] <- dim(X) [1] / sig - crossprod(Y - X %*% beta) / (sig^3)
return ( grad )
}

#initial values - start at uniform random numbers equal to number of coefficients
theta0 <- runif(dim(X)[2]+1)

#algorithm parameters
options.3 <- list("algorithm" = "NLOPT_LN_NELDERMEAD", "xtol_rel" = 1.0e-6, "maxeval" = 1e4)

#optimize
results.3 <- nloptr( x0 = theta0, eval_f = obj.func.2, opts = options.3, Y = Y, X = X)
print(results.3)

beta.hat <- results.3$solution[1:(length(results.3$solution)-1)]
sigma.hat <- results.3$solution[length(results.3$solution)]

```

Question 9 - Compute Beta OLS using lm()
```{r}
#import stargazer
library("stargazer")

#compute B-OLS using built in lm function
beta.lm <- lm(Y ~ X - 1)
beta.lm

stargazer(beta.lm)
```