---
title: "PS9_Thatcher"
author: "Rachel Thatcher"
date: "March 29, 2018"
output: pdf_document
---

3. Install maching learning packages
```{r}
#load packages
library("mlr")
library("glmnet")
```

4. Load the housing data
```{r}
#load the housing data from UCI
housing <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data")
names(housing) <- c("crim","zn","indus","chas","nox","rm","age","dis","rad","tax","ptratio","b","lstat","medv")
```

5. Add new feature to the data set
```{r}
#create a sixth degree polynomials of each of the features, as well as 3rd degree interactions of each
housing$lmedv <- log(housing$medv)
housing$medv <- NULL #drops the median value

formula <- as.formula(lmedv ~ .^3 + 
                        poly(crim, 6) +
                        poly(zn, 6) +
                        poly(indus, 6) +
                        poly(nox, 6) +
                        poly(rm, 6) +
                        poly(age, 6) +
                        poly(dis, 6) +
                        poly(rad, 6) +
                        poly(tax, 6) +
                        poly(ptratio, 6) +
                        poly(b, 6) +
                        poly(lstat, 6))
mod_matrix <- data.frame(model.matrix(formula, housing))

#now replace the intercept column by the response since MLR will do "y - ." and get the intercept by default
mod_matrix[ , 1] = housing$lmedv
colnames(mod_matrix)[1] = "lmedv" #make sure to rename it otherwise MLR won't find it
head(mod_matrix) #just make sure everything is good

#break up the data:
n <- nrow(mod_matrix)
train <- sample(n, size = .8*n)
test <- setdiff(1:n, train)
housing.train <- mod_matrix[train, ]
housing.test <- mod_matrix[test, ]

#define the task
the.task <- makeRegrTask(id = "taskname", data = housing.train, target = "lmedv")

#set resampling strategy (6-fold CV)
resample.strat <- makeResampleDesc(method = "CV", iters = 6)

```


6. Estimate a LASSO model to predict log median house value
```{r}
#we will be using the 'glmnet' package
#create a new prediction algorithm
p.alg <- makeLearner("regr.glmnet")

#search over penalty parameter lambda and force elastic net paramete to be 1 (LASSO)
model.params <- makeParamSet(makeNumericParam("lambda", lower = 0, upper = 1), makeNumericParam("alpha", lower = 1, upper = 1))

#take 50 ransom guesses at lambda within the interval specified above
tune.method <- makeTuneControlRandom(maxit = 50L)

#do the tuning
tuned.model <- tuneParams(learner = p.alg,
                          task = the.task,
                          resampling = resample.strat,
                          measures = rmse, #RMSE performance measure, this can be changed to one or many
                          par.set = model.params,
                          control = tune.method,
                          show.info = TRUE)

#apply the optimal algorithm parameters to the model
p.alg <- setHyperPars(learner = p.alg, par.vals = tuned.model$x)

#verify performance on cross calidated sample sets
resample(p.alg, the.task, resample.strat, measures = list(rmse))

#train the final model 
final.model <- train(learner = p.alg, task = the.task)

#predict in test set
prediction <- predict(final.model, newdata = housing.test)

print(head(prediction$data))

#finds out of sample RMSE
performance(prediction, measures = list(rmse))
```

7. Ridge Regression
```{r}
#search over penalty parameter lambda and force elastic net parameter to be 0 (ridge)
model.params <- makeParamSet(makeNumericParam("lambda", lower = 0, upper = 1), makeNumericParam("alpha", lower = 0, upper = 0))

#do tuning again
tuned.model <- tuneParams(learner = p.alg,
                          task = the.task,
                          resampling = resample.strat,
                          measures = rmse,
                          par.set = model.params,
                          control = tune.method,
                          show.info = TRUE)

#apply the optimal algorithm parameters to the model
p.alg <- setHyperPars(learner = p.alg, par.vals = tuned.model$x)

#verify performance on cross validated sample sets
resample(p.alg, the.task, resample.strat, measures = list(rmse))

#train the final model
final.model <- train(learner = p.alg, task = the.task)

#predict in test set
prediction <- predict(final.model, newdata = housing.test)

print(head(prediction$data))

#finds out of sample RMSE
performance(prediction, measures = list(rmse))
```

8. Estimate the elastic net model
```{r}
#search over penalty parameter lambda and force elastic net parameter to be 0 (ridge)
model.params <- makeParamSet(makeNumericParam("lambda", lower = 0, upper = 1), makeNumericParam("alpha", lower = 0, upper = 1))

#do tuning again
tuned.model <- tuneParams(learner = p.alg,
                          task = the.task,
                          resampling = resample.strat,
                          measures = rmse,
                          par.set = model.params,
                          control = tune.method,
                          show.info = TRUE)

#apply the optimal algorithm parameters to the model
p.alg <- setHyperPars(learner = p.alg, par.vals = tuned.model$x)

#verify performance on cross validated sample sets
resample(p.alg, the.task, resample.strat, measures = list(rmse))

#train the final model
final.model <- train(learner = p.alg, task = the.task)

#predict in test set
prediction <- predict(final.model, newdata = housing.test)

print(head(prediction$data))

#finds out of sample RMSE
performance(prediction, measures = list(rmse))
```